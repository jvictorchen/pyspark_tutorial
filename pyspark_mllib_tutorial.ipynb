{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Vector\n",
    "A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib recognizes the following types as dense vectors:\n",
    "\n",
    "NumPy’s [array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)\n",
    "\n",
    "Python’s list, e.g., [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([0.0, 1.0, 2.0]),\n",
       " DenseVector([0.0, 1.0, 2.0]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# create through numpy array\n",
    "v1 = Vectors.dense(np.array([1.0, 2.0, 3.0]))\n",
    "# create through Python's list\n",
    "v2 = Vectors.dense([0, 1, 2])\n",
    "# create through \n",
    "v3 = Vectors.dense(0, 1, 2)\n",
    "v1, v2, v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following as sparse vectors:\n",
    "\n",
    "MLlib’s [SparseVector](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector).\n",
    "\n",
    "SciPy’s [csc_matrix (Compressed Sparse Column matrix)](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix) with a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SparseVector(3, {0: 1.0, 2: 3.0}),\n",
       " <3x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2 stored elements in Compressed Sparse Column format>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Create a SparseVector through factory methods.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "# Create a SparseVector through scipy's csc_matrix\n",
    "sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape=(3, 1))\n",
    "sv1, sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented in Vectors to create sparse vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabeledPoint\n",
    "A labeled point is a local vector, either dense or sparse, associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms. We use a double to store a label, so we can use labeled points in both regression and classification. For binary classification, a label should be either 0 (negative) or 1 (positive). For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LabeledPoint(1.0, [1.0,0.0,3.0]), LabeledPoint(0.0, (3,[0,2],[1.0,3.0])))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, Vectors.sparse(3, [0, 2], [1.0, 3.0]))\n",
    "pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local matrix\n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. MLlib supports dense matrices, whose entry values are stored in a single double array in column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order. For example, the following dense matrix\n",
    "\n",
    "\\begin{pmatrix}\n",
    "1.0 & 2.0 \\\\\n",
    "3.0 & 4.0 \\\\\n",
    "5.0 & 6.0\n",
    "\\end{pmatrix}\n",
    "\n",
    "is stored in a one-dimensional array [1.0, 3.0, 5.0, 2.0, 4.0, 6.0] with the matrix size (3, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base class of local matrices is Matrix, and we provide two implementations: DenseMatrix, and SparseMatrix. We recommend using the factory methods implemented in Matrices to create local matrices. Remember, local matrices in MLlib are stored in column-major order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False),\n",
       " SparseMatrix(3, 2, [0, 1, 3], [0, 2, 1], [9.0, 6.0, 8.0], False))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
    "dm2, sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Matrix\n",
    "A distributed matrix has long-typed row and column indices and double-typed values, stored distributively in one or more RDDs. It is very important to choose the right format to store large and distributed matrices. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive. Four types of distributed matrices have been implemented so far.\n",
    "\n",
    "The basic type is called [RowMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix). A RowMatrix is a row-oriented distributed matrix without meaningful row indices, e.g., a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector. We assume that the number of columns is not huge for a RowMatrix so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node. \n",
    "\n",
    "An [IndexedRowMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix) is similar to a RowMatrix but with row indices, which can be used for identifying rows and executing joins. \n",
    "\n",
    "A [CoordinateMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.CoordinateMatrix) is a distributed matrix stored in coordinate list (COO) format, backed by an RDD of its entries. \n",
    "\n",
    "A [BlockMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.BlockMatrix) is a distributed matrix backed by an RDD of MatrixBlock which is a tuple of (Int, Int, Matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RowMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices, backed by an RDD of its rows, where each row is a local vector. Since each row is represented by a local vector, the number of columns is limited by the integer range but it should be much smaller in practice.\n",
    "\n",
    "A [RowMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix) can be created from an RDD of vectors.\n",
    "\n",
    "Refer to the [RowMatrix Python docs](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix) for more details on the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is different from `pandas.DataFrame`!**\n",
    "\n",
    "[DataFrame](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes) is a new data structure introduced to support `spark.ml` library. I find a good summary of the differences between `ml` and `mllib` in http://yuqli.com/?p=2330. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|1.218| [1.56,-0.605]|\n",
      "|2.949| [0.346,2.158]|\n",
      "|3.627|  [1.38,0.231]|\n",
      "|0.273|  [0.52,1.151]|\n",
      "|4.199|[0.795,-0.226]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that we're using the ml version of Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.218, Vectors.dense(1.560, -0.605)),\n",
    "    (2.949, Vectors.dense(0.346, 2.158)),\n",
    "    (3.627, Vectors.dense(1.380, 0.231)),\n",
    "    (0.273, Vectors.dense(0.520, 1.151)),\n",
    "    (4.199, Vectors.dense(0.795, -0.226))], [\"label\", \"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be using the same data set used by The Elements of Statistical Machine Learning, which comes from a study by Stamey et al. (1989) that examined the correlation between the level of prostate specific antigen (PSA) and a number of clinical measures, in 97 men who were about to receive a radical prostatectonmy.\n",
    "\n",
    "Through this simple example, we hope you can learn the following topics:\n",
    "  * Read text data into RDD of `LabeledPoint`s and `DataFrame`;\n",
    "  * View basic statistics such as correlation matrix;\n",
    "  * Transform data using `StandardScaler`;\n",
    "  * Train a regression model using `LinearRegression`;\n",
    "  * Train a regression model with regularization (e.g. Ridge), and use cross-validation to choose the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, we will download the data file and save it locally. The file has the following format: \n",
    "```\n",
    "        lcavol  lweight age     lbph    svi     lcp     gleason pgg45   lpsa    train\n",
    "1       -0.579818495    2.769459        50      -1.38629436     0       -1.38629436     6         0     -0.4307829      T\n",
    "2       -0.994252273    3.319626        58      -1.38629436     0       -1.38629436     6         0     -0.1625189      T\n",
    "3       -0.510825624    2.691243        74      -1.38629436     0       -1.38629436     7        20     -0.1625189      T\n",
    "4       -1.203972804    3.282789        58      -1.38629436     0       -1.38629436     6         0     -0.1625189      T\n",
    "5        0.751416089    3.432373        62      -1.38629436     0       -1.38629436     6         0      0.3715636      T\n",
    "6       -1.049822124    3.228826        50      -1.38629436     0       -1.38629436     6         0      0.7654678      T\n",
    "```\n",
    "\n",
    "There're 8 predictors (column 1--8) and the outcome is `lpsa` (column 9). This last column indicates which 67 observations were used as the \n",
    "\"training set\" and which 30 as the test set, as described on page 48\n",
    "in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n",
      "Total rows: 97\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# retrieve the data and save it locally\n",
    "f = urlretrieve(\"https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data\", \n",
    "                \"prostate.data\")\n",
    "\n",
    "# read data as LabeledPoint RDDs\n",
    "def parse_data_point(row):\n",
    "    values = row.split('\\t')\n",
    "    is_train = values[-1] == 'T'\n",
    "    label = float(values[-2])\n",
    "    features = [float(v) for v in values[1:-2]] # skip the id column\n",
    "    return is_train, LabeledPoint(label, features)\n",
    "\n",
    "# read psa data into RDD of strings\n",
    "psa_file = sc.textFile('prostate.data')\n",
    "# skip the header row\n",
    "header = psa_file.first()\n",
    "# split the first row to get the list of header names\n",
    "feature_labels = header.split('\\t')[1:-2]\n",
    "print('Predictors: %s' % (feature_labels))\n",
    "\n",
    "psa_data = psa_file.filter(lambda x: x != header).map(parse_data_point)\n",
    "print('Total rows: %s' % psa_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizes features by scaling to unit variance and/or removing the mean using column summary statistics on the samples in the training set. This is a very common pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "label = psa_data.map(lambda x: x[1].label)\n",
    "features = psa_data.map(lambda x: x[1].features)\n",
    "is_train = psa_data.map(lambda x: x[0])\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "# use the scaler to transform the original data\n",
    "std_psa_data = label.zip(scaler.transform(features.map(lambda x: Vectors.dense(x.toArray())))) \\\n",
    "               .map(lambda x: LabeledPoint(x[0], x[1]))\n",
    "# add the is_train data back\n",
    "std_psa_data = is_train.zip(std_psa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training/test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we would use `data.randomSplit` to split the data into training and test sets. Since the data contains the split information, we'll just use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data_training = std_psa_data.filter(lambda x: x[0]).map(lambda x: x[1])\n",
    "data_test = std_psa_data.filter(lambda x: not x[0]).map(lambda x: x[1])\n",
    "\n",
    "df_data_training = data_training.map(\n",
    "    lambda x: [x.label, Vectors.dense(x.features.toArray())]\n",
    ").toDF(['label', 'features'])\n",
    "\n",
    "df_data_test = data_test.map(\n",
    "    lambda x: [x.label, Vectors.dense(x.features.toArray())]\n",
    ").toDF(['label', 'features'])\n",
    "\n",
    "\n",
    "data_training.count(), data_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  0.3  ,  0.286,  0.063,  0.593,  0.692,  0.426,  0.483],\n",
       "       [ 0.3  ,  1.   ,  0.317,  0.437,  0.181,  0.157,  0.024,  0.074],\n",
       "       [ 0.286,  0.317,  1.   ,  0.287,  0.129,  0.173,  0.366,  0.276],\n",
       "       [ 0.063,  0.437,  0.287,  1.   , -0.139, -0.089,  0.033, -0.03 ],\n",
       "       [ 0.593,  0.181,  0.129, -0.139,  1.   ,  0.671,  0.307,  0.481],\n",
       "       [ 0.692,  0.157,  0.173, -0.089,  0.671,  1.   ,  0.476,  0.663],\n",
       "       [ 0.426,  0.024,  0.366,  0.033,  0.307,  0.476,  1.   ,  0.757],\n",
       "       [ 0.483,  0.074,  0.276, -0.03 ,  0.481,  0.663,  0.757,  1.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "np.round(Statistics.corr(data_training.map(lambda x: x.features)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since [LinearRegressionWithSGD](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD) has been deprecated in spark 2.0.0, we will need to use `ml.regression.LinearRegression` instead, which requires us to convert the input into `DataFrame` format. \n",
    "\n",
    "We will then show how to use `LinearRegression` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.57   0.257 -0.107  0.199  0.279 -0.151  0.015  0.194]\n",
      "Intercept: 2.467\n",
      "\n",
      "\n",
      "Training Summary:\n",
      "numIterations: 1\n",
      "objectiveHistory: [0.0]\n",
      "MSE = 0.449112\n",
      "RMSE = 0.670158\n",
      "R-squared = 0.687474\n",
      "MAE = 0.506446\n",
      "Explained variance = 0.883040\n",
      "\n",
      "\n",
      "Test Summary:\n",
      "MSE = 0.492781\n",
      "Std Error = 0.162808\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# no regularization\n",
    "reg_param = 0.1\n",
    "lr = LinearRegression(maxIter=10, regParam=reg_param)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(df_data_training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(np.round(lrModel.coefficients, 3)))\n",
    "print(\"Intercept: %s\" % str(round(lrModel.intercept, 3)))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print('\\n')\n",
    "print('Training Summary:')\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print('MSE = %f' % trainingSummary.meanSquaredError)\n",
    "print(\"RMSE = %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R-squared = %f\" % trainingSummary.r2)\n",
    "print(\"MAE = %f\" % trainingSummary.meanAbsoluteError)\n",
    "print(\"Explained variance = %f\" % trainingSummary.explainedVariance)\n",
    "\n",
    "# Prediction\n",
    "lrPred = lrModel.transform(df_data_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(lrPred, {evaluator.metricName: \"mse\"})\n",
    "sde = (lrPred.rdd.map(lambda x: (x.prediction - x.label)**2).variance() / (lrPred.count() - 1))**0.5\n",
    "print('\\n')\n",
    "print('Test Summary:')\n",
    "print(\"MSE = %g\" % mse)\n",
    "print('Std Error = %g' % sde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression (with tuning)\n",
    "Now let's introduce the Ridge regularizor and use cross-validation to tune the hyperparams. \n",
    "To get the list of regparam to iterate through, we will need to solve $\\lambda$ in the formula \n",
    "$$df(\\lambda) = \\sum_{i=1}^p\\frac{d_i^2}{d_i^2+\\lambda}$$\n",
    "where $d_i$ is the $i$th diagonal entry of the matrix $D$ in singular value decomposition of the features matrix $X$\n",
    "$$X = UDV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5917807097034505e-15,\n",
       " 4.87448572221687,\n",
       " 12.325917405547605,\n",
       " 24.176282072356294,\n",
       " 44.21483690462158,\n",
       " 81.5530281510686,\n",
       " 164.0241749168161,\n",
       " 431.60164542313055]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_optimal_lambdas(data_training.map(lambda x: x.features), multiple=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RowMatrix(data_training.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([-1.6374, -2.0062, -1.8624, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-1.989, -0.722, -0.7879, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-1.5788, -2.1888, 1.3612, -1.0247, -0.5229, -0.8632, 0.3426, -0.1553]),\n",
       " DenseVector([-2.1669, -0.808, -0.7879, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.5079, -0.4588, -0.2506, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-2.0361, -0.934, -1.8624, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.5573, -0.2088, -0.7879, 0.9901, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.9294, -0.0579, 0.1523, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-2.2883, -0.0706, -0.1163, 0.8041, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([0.2235, -1.4147, -0.1163, -1.0247, -0.5229, -0.2993, 0.3426, 0.1992]),\n",
       " DenseVector([0.1078, -1.4722, 0.4209, -1.0247, -0.5229, -0.8632, 0.3426, -0.6872]),\n",
       " DenseVector([0.1622, -1.3256, 0.2866, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-1.498, -0.2636, 0.8239, 0.7884, -0.5229, -0.2993, 0.3426, 0.1992]),\n",
       " DenseVector([0.7962, 0.0477, 0.2866, -1.0247, -0.5229, 0.394, -1.0422, -0.8645]),\n",
       " DenseVector([-1.6223, -0.8433, -3.0713, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.9907, 0.4585, 0.8239, 1.0738, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.1719, -0.4892, -0.6536, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-1.6076, -0.5907, -0.6536, -0.6196, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([0.3663, -0.414, -0.1163, 0.2329, -0.5229, 0.9712, 0.3426, 1.2629]),\n",
       " DenseVector([-0.7103, 0.2117, 0.1523, -1.0247, -0.5229, -0.4428, 0.3426, 1.6174]),\n",
       " DenseVector([-0.2628, -1.1671, 0.4209, 0.0846, -0.5229, 0.1632, 0.3426, 1.972]),\n",
       " DenseVector([0.899, -0.5907, 0.1523, -1.0247, -0.5229, 1.2864, -1.0422, -0.8645]),\n",
       " DenseVector([-0.9035, 1.0766, 0.1523, 1.2838, -0.5229, -0.4428, -1.0422, -0.8645]),\n",
       " DenseVector([-0.0633, -1.3809, 0.9582, 0.8041, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-1.1539, -0.9619, -0.1163, -1.0247, -0.5229, -0.4428, -1.0422, -0.8645]),\n",
       " DenseVector([0.062, 0.0658, 1.2268, -0.4688, -0.5229, 1.3142, 1.7274, -0.3326]),\n",
       " DenseVector([-0.7573, -2.9272, 0.018, -1.0247, -0.5229, -0.8632, 0.3426, -0.3326]),\n",
       " DenseVector([1.1123, 1.0648, 0.5553, 0.8777, 1.8925, 1.4389, 0.3426, 0.3765]),\n",
       " DenseVector([-0.4688, -1.4375, -1.0565, 0.5761, -0.5229, 0.012, 0.3426, -0.6872]),\n",
       " DenseVector([-0.6189, -1.1366, -0.5193, -1.0247, -0.5229, -0.8632, 3.1122, 1.972]),\n",
       " DenseVector([-0.6514, 0.5533, -0.2506, 1.1121, -0.5229, -0.1798, -1.0422, -0.8645]),\n",
       " DenseVector([0.1155, -0.5122, 0.2866, 1.1365, -0.5229, -0.1798, 0.3426, -0.1553]),\n",
       " DenseVector([0.2663, -0.5511, -0.3849, 0.3549, -0.5229, -0.8632, 0.3426, -0.3326]),\n",
       " DenseVector([1.169, 0.8555, 2.0327, 1.2263, 1.8925, 2.0283, 3.1122, 2.6811]),\n",
       " DenseVector([-0.219, 0.8512, 0.5553, -1.0247, -0.5229, -0.8632, 0.3426, 0.9083]),\n",
       " DenseVector([0.2631, 1.4143, 0.018, 1.3598, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.0705, 1.52, 0.2866, 1.3936, -0.5229, -0.8632, 0.3426, -0.3326]),\n",
       " DenseVector([-0.752, 0.3168, -1.9967, 0.9117, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.6853, 1.2821, 0.8239, 0.2329, -0.5229, -0.8632, 0.3426, -0.1553]),\n",
       " DenseVector([-0.245, 0.5188, -0.3849, 0.8232, -0.5229, -0.8632, 0.3426, 0.5538]),\n",
       " DenseVector([-0.7573, 2.0904, 1.2268, 1.5343, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([1.2096, -0.2429, 1.0925, -1.0247, -0.5229, 1.2426, 3.1122, 2.5038]),\n",
       " DenseVector([0.5709, 0.5824, 0.5553, 1.1601, -0.5229, 1.0736, 0.3426, 1.6174]),\n",
       " DenseVector([0.7198, 0.985, 1.0925, 1.5214, -0.5229, -0.1798, 0.3426, -0.5099]),\n",
       " DenseVector([-1.5241, 1.8198, 0.6896, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([-0.1324, 2.6877, 1.0925, 1.5343, -0.5229, -0.4428, 0.3426, -0.6872]),\n",
       " DenseVector([0.4362, -0.0834, -0.5193, -1.0247, 1.8925, 1.0736, 0.3426, 1.2629]),\n",
       " DenseVector([-0.1612, -0.6719, 1.7641, 1.1365, -0.5229, -0.8632, 0.3426, 0.0219]),\n",
       " DenseVector([1.3993, 0.5139, 0.6896, -1.0247, 1.8925, 1.4939, 0.3426, -0.1553]),\n",
       " DenseVector([1.5197, -0.8522, 0.5553, -0.1045, 1.8925, 1.8593, 0.3426, 0.9083]),\n",
       " DenseVector([0.5607, 1.8787, 1.0925, 1.3936, -0.5229, 0.4864, 0.3426, 1.2629]),\n",
       " DenseVector([1.0077, 1.6943, 1.8984, 1.5343, -0.5229, -0.8632, 0.3426, -0.5099]),\n",
       " DenseVector([1.1015, -0.1093, 0.6896, -1.0247, 1.8925, 1.9763, 0.3426, 1.6174]),\n",
       " DenseVector([0.1, -1.3038, 0.2866, 0.3166, -0.5229, 0.2879, 0.3426, 0.5538]),\n",
       " DenseVector([0.9873, -0.3628, -0.9222, 0.2329, -0.5229, 1.7927, 0.3426, 1.2629]),\n",
       " DenseVector([1.0716, 0.6065, 1.7641, -0.4329, 1.8925, 0.5285, 0.3426, 0.1992]),\n",
       " DenseVector([0.1802, 0.189, -0.5193, 1.0996, -0.5229, 0.7082, 0.3426, 0.1992]),\n",
       " DenseVector([1.6569, -0.2567, 0.018, -1.0247, 1.8925, 1.7927, 0.3426, 1.2629]),\n",
       " DenseVector([0.572, 0.2399, -0.7879, 1.0605, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([0.3238, -0.6067, -0.2506, -1.0247, 1.8925, 0.3429, 0.3426, 0.1992]),\n",
       " DenseVector([1.2367, 2.5422, 0.1523, -1.0247, 1.8925, 1.8904, 0.3426, 1.2629]),\n",
       " DenseVector([0.1802, 0.1544, 1.6298, 0.5761, 1.8925, 0.7082, 0.3426, 1.7947]),\n",
       " DenseVector([1.6091, 1.1038, 0.5553, -1.0247, -0.5229, -0.8632, -1.0422, -0.8645]),\n",
       " DenseVector([1.0036, 0.1135, -0.3849, 0.86, 1.8925, -0.8632, 0.3426, -0.3326]),\n",
       " DenseVector([1.2559, 0.5776, 0.5553, -1.0247, 1.8925, 1.0736, 0.3426, 1.2629]),\n",
       " DenseVector([2.0965, 0.6255, -2.6683, -1.0247, 1.8925, 1.6795, 0.3426, 0.5538]),\n",
       " DenseVector([1.3003, 0.3384, 0.5553, 1.0048, 1.8925, 1.2426, 0.3426, 1.972])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training.map(lambda x: x.features).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = m.computeSVD(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.000000024611741"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((svd.s*svd.s) / (svd.s*svd.s + 12.325917405547605))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "def get_optimal_lambdas(features, multiple=1):\n",
    "    \"\"\"Return a list of optimal lambdas for cross-validation\"\"\"\n",
    "    p = len(features.first())\n",
    "    svd = RowMatrix(features).computeSVD(p)\n",
    "    dj = svd.s # the diagional elements d_j\n",
    "    k_range = np.arange(p, 0, -1.0 / multiple)\n",
    "    return [solve_lambda(dj, k) for k in k_range]\n",
    "    \n",
    "def solve_lambda(dj, df):\n",
    "    \"\"\"Given the diagonal elements d_j, and the degree of freedom, solve for lambda\"\"\"\n",
    "    xn = 1.0 # initial guess\n",
    "    f = np.sum(dj * dj / (dj * dj + xn)) - df\n",
    "    fp = - np.sum( dj * dj / (( dj * dj + xn ) * ( dj * dj + xn )))\n",
    "    xnp1 = xn - f / fp \n",
    "    \n",
    "    while abs(xn - xnp1) / abs(xn) > 0.0001:\n",
    "        xn = xnp1\n",
    "        f = np.sum(dj * dj / (dj * dj + xn)) - df\n",
    "        fp = - np.sum( dj * dj / (( dj * dj + xn ) * ( dj * dj + xn ))) \n",
    "        xnp1 = xn - f / fp \n",
    "\n",
    "    return xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambdas: [1.5917807097034505e-15, 4.87448572221687, 12.325917405547605, 24.176282072356294, 44.21483690462158, 81.5530281510686, 164.0241749168161, 431.60164542313055]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Ridge regression (by setting elasticNetParam = 0)\n",
    "rr = LinearRegression(maxIter=10, elasticNetParam=0.0)\n",
    "\n",
    "lambdas = get_optimal_lambdas(data_training.map(lambda x: x.features))\n",
    "print('Optimal lambdas: {}'.format(lambdas))\n",
    "\n",
    "# Here we just make a few wild guesses of lambdas\n",
    "# It's better to calculate the optimal lambdas by inferring them from effective degree of freeom\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rr.regParam, lambdas) \\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(estimator=rr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName='mse'),\n",
    "                          numFolds=10)\n",
    "# Fit the model\n",
    "rrModel = crossval.fit(df_data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.68   0.263 -0.141  0.21   0.305 -0.288 -0.021  0.267]\n",
      "Intercept: 2.465\n",
      "\n",
      "\n",
      "Test Summary:\n",
      "MSE = 0.521274\n",
      "Std Error = 0.17572\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(np.round(rrModel.bestModel.coefficients, 3)))\n",
    "print(\"Intercept: %s\" % str(round(rrModel.bestModel.intercept, 3)))\n",
    "\n",
    "# Prediction\n",
    "rrPred = rrModel.transform(df_data_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(rrPred, {evaluator.metricName: \"mse\"})\n",
    "sde = (rrPred.rdd.map(lambda x: (x.prediction - x.label)**2).variance() / rrPred.count())**0.5\n",
    "print('\\n')\n",
    "print('Test Summary:')\n",
    "print(\"MSE = %g\" % mse)\n",
    "print('Std Error = %g' % sde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main concepts in Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. This section covers the key concepts introduced by the Pipelines API, where the pipeline concept is mostly inspired by the scikit-learn project.\n",
    "\n",
    "[DataFrame](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#dataframe): This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n",
    "\n",
    "[Transformer](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#transformers): A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "[Estimator](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#estimators): An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "[Pipeline](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#pipeline): A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "[Parameter](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#parameters): All Transformers and Estimators now share a common API for specifying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "![Pipeline](https://spark.apache.org/docs/2.2.0/img/ml-Pipeline.png)\n",
    "\n",
    "### PipelineModel\n",
    "![PipelineModel](https://spark.apache.org/docs/2.2.0/img/ml-PipelineModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    " * [Data Types - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-data-types.html)\n",
    " * [Linear Methods - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#mjx-eqn-eqregPrimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
