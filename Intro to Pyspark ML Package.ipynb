{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML vs. MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're two machine learning packages in PySpark, `ml` and `mllib`. `pyspark.ml` is an older library based on RDD, whereas `mllib` is a newer library based on `DataFrame`. **We recommend using `ml` whenever it is possible, and only use `mllib` is the needed feature doesn't exist in `ml`.** More differences between `ml` and `mllib` are summarized in this post: http://yuqli.com/?p=2330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Vector\n",
    "A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values. For example, a vector (1.0, 0.0, 3.0) can be represented in dense format as [1.0, 0.0, 3.0] or in sparse format as (3, [0, 2], [1.0, 3.0]), where 3 is the size of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib recognizes the following types as dense vectors:\n",
    "\n",
    "NumPy’s [array](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)\n",
    "\n",
    "Python’s list, e.g., [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([0.0, 1.0, 2.0]),\n",
       " DenseVector([0.0, 1.0, 2.0]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# create through numpy array\n",
    "v1 = Vectors.dense(np.array([1.0, 2.0, 3.0]))\n",
    "# create through Python's list\n",
    "v2 = Vectors.dense([0, 1, 2])\n",
    "# create through \n",
    "v3 = Vectors.dense(0, 1, 2)\n",
    "v1, v2, v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following as sparse vectors:\n",
    "\n",
    "MLlib’s [SparseVector](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector).\n",
    "\n",
    "SciPy’s [csc_matrix (Compressed Sparse Column matrix)](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix) with a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SparseVector(3, {0: 1.0, 2: 3.0}),\n",
       " <3x1 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 2 stored elements in Compressed Sparse Column format>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Create a SparseVector through factory methods.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "# Create a SparseVector through scipy's csc_matrix\n",
    "sv2 = sps.csc_matrix((np.array([1.0, 3.0]), np.array([0, 2]), np.array([0, 2])), shape=(3, 1))\n",
    "sv1, sv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented in Vectors to create sparse vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabeledPoint\n",
    "A labeled point is a local vector, either dense or sparse, associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms. We use a double to store a label, so we can use labeled points in both regression and classification. For binary classification, a label should be either 0 (negative) or 1 (positive). For multiclass classification, labels should be class indices starting from zero: 0, 1, 2, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LabeledPoint(1.0, [1.0,0.0,3.0]), LabeledPoint(0.0, (3,[0,2],[1.0,3.0])))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, Vectors.sparse(3, [0, 2], [1.0, 3.0]))\n",
    "pos, neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local matrix\n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. MLlib supports dense matrices, whose entry values are stored in a single double array in column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order. For example, the following dense matrix\n",
    "\n",
    "\\begin{pmatrix}\n",
    "1.0 & 2.0 \\\\\n",
    "3.0 & 4.0 \\\\\n",
    "5.0 & 6.0\n",
    "\\end{pmatrix}\n",
    "\n",
    "is stored in a one-dimensional array [1.0, 3.0, 5.0, 2.0, 4.0, 6.0] with the matrix size (3, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base class of local matrices is Matrix, and we provide two implementations: DenseMatrix, and SparseMatrix. We recommend using the factory methods implemented in Matrices to create local matrices. Remember, local matrices in MLlib are stored in column-major order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False),\n",
       " SparseMatrix(3, 2, [0, 1, 3], [0, 2, 1], [9.0, 6.0, 8.0], False))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n",
    "dm2, sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Matrix\n",
    "A distributed matrix has long-typed row and column indices and double-typed values, stored distributively in one or more RDDs. It is very important to choose the right format to store large and distributed matrices. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive. Four types of distributed matrices have been implemented so far.\n",
    "\n",
    "The basic type is called [RowMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix). A RowMatrix is a row-oriented distributed matrix without meaningful row indices, e.g., a collection of feature vectors. It is backed by an RDD of its rows, where each row is a local vector. We assume that the number of columns is not huge for a RowMatrix so that a single local vector can be reasonably communicated to the driver and can also be stored / operated on using a single node. \n",
    "\n",
    "An [IndexedRowMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.IndexedRowMatrix) is similar to a RowMatrix but with row indices, which can be used for identifying rows and executing joins. \n",
    "\n",
    "A [CoordinateMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.CoordinateMatrix) is a distributed matrix stored in coordinate list (COO) format, backed by an RDD of its entries. \n",
    "\n",
    "A [BlockMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.BlockMatrix) is a distributed matrix backed by an RDD of MatrixBlock which is a tuple of (Int, Int, Matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RowMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices, backed by an RDD of its rows, where each row is a local vector. Since each row is represented by a local vector, the number of columns is limited by the integer range but it should be much smaller in practice.\n",
    "\n",
    "A [RowMatrix](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix) can be created from an RDD of vectors.\n",
    "\n",
    "Refer to the [RowMatrix Python docs](https://spark.apache.org/docs/2.2.0/api/python/pyspark.mllib.html#pyspark.mllib.linalg.distributed.RowMatrix) for more details on the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.linalg.distributed.RowMatrix at 0x7f2fcc4dc890>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is different from `pandas.DataFrame`!**\n",
    "\n",
    "[DataFrame](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes) is a new data structure introduced to support `spark.ml` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|1.218| [1.56,-0.605]|\n",
      "|2.949| [0.346,2.158]|\n",
      "|3.627|  [1.38,0.231]|\n",
      "|0.273|  [0.52,1.151]|\n",
      "|4.199|[0.795,-0.226]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that we're using the ml version of Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.218, Vectors.dense(1.560, -0.605)),\n",
    "    (2.949, Vectors.dense(0.346, 2.158)),\n",
    "    (3.627, Vectors.dense(1.380, 0.231)),\n",
    "    (0.273, Vectors.dense(0.520, 1.151)),\n",
    "    (4.199, Vectors.dense(0.795, -0.226))], [\"label\", \"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Regression Example - Least Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be using the same data set used by The Elements of Statistical Machine Learning, which comes from a study by Stamey et al. (1989) that examined the correlation between the level of prostate specific antigen (PSA) and a number of clinical measures, in 97 men who were about to receive a radical prostatectonmy.\n",
    "\n",
    "Through this simple example, we hope you can learn the following topics:\n",
    "  * Read text data into `DataFrame`;\n",
    "  * View basic statistics such as correlation matrix;\n",
    "  * Transform data using `StandardScaler`;\n",
    "  * Train a regression model using `LinearRegression`;\n",
    "  \n",
    "#### Note that this tutorial is based on `pyspark.ml`, not `pyspark.mllib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, we will download the data file and save it locally. The file has the following format: \n",
    "```\n",
    "        lcavol  lweight age     lbph    svi     lcp     gleason pgg45   lpsa    train\n",
    "1       -0.579818495    2.769459        50      -1.38629436     0       -1.38629436     6         0     -0.4307829      T\n",
    "2       -0.994252273    3.319626        58      -1.38629436     0       -1.38629436     6         0     -0.1625189      T\n",
    "3       -0.510825624    2.691243        74      -1.38629436     0       -1.38629436     7        20     -0.1625189      T\n",
    "4       -1.203972804    3.282789        58      -1.38629436     0       -1.38629436     6         0     -0.1625189      T\n",
    "5        0.751416089    3.432373        62      -1.38629436     0       -1.38629436     6         0      0.3715636      T\n",
    "6       -1.049822124    3.228826        50      -1.38629436     0       -1.38629436     6         0      0.7654678      T\n",
    "```\n",
    "\n",
    "There're 8 predictors (column 1--8) and the outcome is `lpsa` (column 9). This last column indicates which 67 observations were used as the \n",
    "\"training set\" and which 30 as the test set, as described on page 48\n",
    "in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import google.datalab.storage as storage\n",
    "bucket = storage.Bucket('mth9898-bucket')\n",
    "\n",
    "# Read data from Google cloud storage\n",
    "f = bucket.object('prostate.data')\n",
    "psa_file = sc.parallelize(f.read_stream().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the first row to get the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\n"
     ]
    }
   ],
   "source": [
    "# skip the header row\n",
    "header = psa_file.first()\n",
    "# split the first row to get the list of header names\n",
    "feature_labels = header.split('\\t')[1:-2]\n",
    "print('Predictors: %s' % (feature_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 97\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# read data as LabeledPoint RDDs\n",
    "def parse_data_point(row):\n",
    "    values = row.split('\\t')\n",
    "    is_train = values[-1] == 'T'\n",
    "    label = float(values[-2])\n",
    "    features = [float(v) for v in values[1:-2]] # skip the id column\n",
    "    return Row(**{\n",
    "        'train': is_train,\n",
    "        'label': label,\n",
    "        'features': Vectors.dense(features)\n",
    "    })\n",
    "\n",
    "df_data = sqlContext.createDataFrame(psa_file.filter(lambda x: x != header).map(parse_data_point))\n",
    "print('Total rows: %s' % df_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|            features|     label|train|\n",
      "+--------------------+----------+-----+\n",
      "|[-0.579818495,2.7...|-0.4307829| true|\n",
      "|[-0.994252273,3.3...|-0.1625189| true|\n",
      "|[-0.510825624,2.6...|-0.1625189| true|\n",
      "|[-1.203972804,3.2...|-0.1625189| true|\n",
      "|[0.751416089,3.43...| 0.3715636| true|\n",
      "|[-1.049822124,3.2...| 0.7654678| true|\n",
      "|[0.737164066,3.47...| 0.7654678|false|\n",
      "|[0.693147181,3.53...| 0.8544153| true|\n",
      "|[-0.776528789,3.5...|  1.047319|false|\n",
      "|[0.223143551,3.24...|  1.047319|false|\n",
      "|[0.254642218,3.60...| 1.2669476| true|\n",
      "|[-1.347073648,3.5...| 1.2669476| true|\n",
      "|[1.613429934,3.02...| 1.2669476| true|\n",
      "|[1.477048724,2.99...| 1.3480731| true|\n",
      "|[1.205970807,3.44...| 1.3987169|false|\n",
      "|[1.541159072,3.06...|  1.446919| true|\n",
      "|[-0.415515444,3.5...| 1.4701758| true|\n",
      "|[2.288486169,3.64...| 1.4929041| true|\n",
      "|[-0.562118918,3.2...| 1.5581446| true|\n",
      "|[0.182321557,3.82...| 1.5993876| true|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training/test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we would use `data.randomSplit` to split the data into training and test sets. Since the data contains the split information, we'll just use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 30)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training = df_data.filter('train')\n",
    "df_testing = df_data.filter('!train')\n",
    "\n",
    "df_training.count(), df_testing.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizes predictors by scaling to unit variance and/or removing the mean using column summary statistics on the samples in the training set. **Note that we need to compute the mean and std on the training set, and use them to transform the testing set. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# scaler (an Estimator)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# Fit the training data to produce a scaler Model (a Transformer)\n",
    "scaler_model = scaler.fit(df_training)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "df_training_scaled = scaler_model.transform(df_training)\n",
    "df_testing_scaled = scaler_model.transform(df_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1.        ,  0.30023199,  0.28632427,  0.06316772,  0.59294913,\n",
      "               0.69204308,  0.42641407,  0.48316136],\n",
      "             [ 0.30023199,  1.        ,  0.31672347,  0.43704154,  0.18105448,\n",
      "               0.15682859,  0.02355821,  0.07416632],\n",
      "             [ 0.28632427,  0.31672347,  1.        ,  0.28734645,  0.12890226,\n",
      "               0.1729514 ,  0.36591512,  0.27580573],\n",
      "             [ 0.06316772,  0.43704154,  0.28734645,  1.        , -0.1391468 ,\n",
      "              -0.08853456,  0.03299215, -0.03040382],\n",
      "             [ 0.59294913,  0.18105448,  0.12890226, -0.1391468 ,  1.        ,\n",
      "               0.67124021,  0.30687537,  0.48135774],\n",
      "             [ 0.69204308,  0.15682859,  0.1729514 , -0.08853456,  0.67124021,\n",
      "               1.        ,  0.47643684,  0.66253335],\n",
      "             [ 0.42641407,  0.02355821,  0.36591512,  0.03299215,  0.30687537,\n",
      "               0.47643684,  1.        ,  0.7570565 ],\n",
      "             [ 0.48316136,  0.07416632,  0.27580573, -0.03040382,  0.48135774,\n",
      "               0.66253335,  0.7570565 ,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "print(str(Correlation.corr(df_training, column='features').collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Regression - a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show how to use `LinearRegression` to train a LS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# no regularization\n",
    "reg_param = 0.0\n",
    "# standardization = False because we already did it\n",
    "lr = LinearRegression(maxIter=10, featuresCol='scaledFeatures', \n",
    "                      regParam=reg_param, standardization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.716  0.293 -0.143  0.212  0.31  -0.289 -0.021  0.277]\n",
      "Intercept: 2.452\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(df_training_scaled)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(np.round(lrModel.coefficients, 3)))\n",
    "print(\"Intercept: %s\" % str(round(lrModel.intercept, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary:\n",
      "numIterations: 1\n",
      "objectiveHistory: [0.0]\n",
      "MSE = 0.439200\n",
      "RMSE = 0.662721\n",
      "R-squared = 0.694371\n",
      "MAE = 0.498614\n",
      "Explained variance = 0.997837\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print('Training Summary:')\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print('MSE = %f' % trainingSummary.meanSquaredError)\n",
    "print(\"RMSE = %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R-squared = %f\" % trainingSummary.r2)\n",
    "print(\"MAE = %f\" % trainingSummary.meanAbsoluteError)\n",
    "print(\"Explained variance = %f\" % trainingSummary.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "|            features|    label|train|      scaledFeatures|        prediction|\n",
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "|[0.737164066,3.47...|0.7654678|false|[-0.4638113238022...|1.9690384442936995|\n",
      "|[-0.776528789,3.5...| 1.047319|false|[-1.6819865855447...|1.1699557741534656|\n",
      "|[0.223143551,3.24...| 1.047319|false|[-0.8774798387409...|1.2611792855419313|\n",
      "|[1.205970807,3.44...|1.3987169|false|[-0.0865295175639...|1.8837591422420163|\n",
      "|[2.059238834,3.50...|1.6582281|false|[0.60015536615859...|2.5443188606403004|\n",
      "|[0.385262401,3.66...|1.7316555|false|[-0.7470113808369...|1.9327540170823725|\n",
      "|[1.446918983,3.12...|1.7664417|false|[0.10737845154330...|2.0423357081761413|\n",
      "|[-0.400477567,3.8...|1.8164521|false|[-1.3793516789484...|1.8309162520003734|\n",
      "|[0.182321557,3.80...| 2.008214|false|[-0.9103321727276...|  1.99115928590372|\n",
      "|[0.009950331,3.26...|2.0215476|false|[-1.0490514397545...|1.3234707565550679|\n",
      "|[1.30833282,4.119...|2.0856721|false|[-0.0041515955390...|2.9384311136674928|\n",
      "|[1.442201993,3.68...|2.3075726|false|[0.10358235743550...|2.2031440372044604|\n",
      "|[1.771556762,3.89...|2.3749058|false|[0.36863733910332...| 2.166420999877872|\n",
      "|[1.16315081,4.035...|2.5687881|false|[-0.1209897852998...| 2.794562370070349|\n",
      "|[1.745715531,3.49...|2.5915164|false|[0.34784108031595...|2.6746687884412164|\n",
      "|[1.220829921,3.56...|2.5915164|false|[-0.0745713418835...|2.1805729056492256|\n",
      "|[0.512823626,3.63...|2.6844403|false|[-0.6443538779364...|2.4021106776703993|\n",
      "|[2.12704052,4.121...|2.6912431|false|[0.65472015869018...| 3.023515762804128|\n",
      "|[3.153590358,3.51...|2.7047113|false|[1.48085712132878...|3.2112228331322754|\n",
      "|[0.97455964,2.865...|2.7880929|false|[-0.2727623837621...|1.3844145898234999|\n",
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "lrPred = lrModel.transform(df_testing_scaled)\n",
    "lrPred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "MSE = 0.521274\n",
      "R squared = 0.50338\n",
      "Std Error = 0.178724\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(lrPred, {evaluator.metricName: \"mse\"})\n",
    "r2 = evaluator.evaluate(lrPred, {evaluator.metricName: \"r2\"})\n",
    "sde = (lrPred.rdd.map(lambda x: (x.prediction - x.label)**2).variance() / (lrPred.count() - 1))**0.5\n",
    "print('Test Summary:')\n",
    "print(\"MSE = %g\" % mse)\n",
    "print(\"R squared = %g\" % r2)\n",
    "print('Std Error = %g' % sde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's introduce regularization (ridge regression) and use cross-validation to select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Ridge regression (by setting elasticNetParam = 0)\n",
    "rr = LinearRegression(maxIter=10, elasticNetParam=0.0, standardization=False)\n",
    "\n",
    "# Here we just make a few wild guesses of lambdas\n",
    "# It's better to calculate the optimal lambdas by inferring them from effective degree of freeom\n",
    "all_lambdas = [0.1, 1.0, 10.0]\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rr.regParam, all_lambdas) \\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(estimator=rr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName='mse'),\n",
    "                          numFolds=10)\n",
    "# Fit the model\n",
    "rrModel = crossval.fit(df_training_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.554  0.444 -0.015  0.154  0.407 -0.11  -0.047  0.009]\n",
      "Intercept: 1.033\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(np.round(rrModel.bestModel.coefficients, 3)))\n",
    "print(\"Intercept: %s\" % str(round(rrModel.bestModel.intercept, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary:\n",
      "numIterations: 1\n",
      "objectiveHistory: [0.0]\n",
      "MSE = 0.458445\n",
      "RMSE = 0.677085\n",
      "R-squared = 0.680979\n",
      "MAE = 0.523436\n",
      "Explained variance = 0.860463\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = rrModel.bestModel.summary\n",
    "print('Training Summary:')\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "print('MSE = %f' % trainingSummary.meanSquaredError)\n",
    "print(\"RMSE = %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"R-squared = %f\" % trainingSummary.r2)\n",
    "print(\"MAE = %f\" % trainingSummary.meanAbsoluteError)\n",
    "print(\"Explained variance = %f\" % trainingSummary.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "|            features|    label|train|      scaledFeatures|        prediction|\n",
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "|[0.737164066,3.47...|0.7654678|false|[-0.4638113238022...|1.9851040528449706|\n",
      "|[-0.776528789,3.5...| 1.047319|false|[-1.6819865855447...|1.1236746647713867|\n",
      "|[0.223143551,3.24...| 1.047319|false|[-0.8774798387409...|1.3062080912389102|\n",
      "|[1.205970807,3.44...|1.3987169|false|[-0.0865295175639...|1.9236425303120877|\n",
      "|[2.059238834,3.50...|1.6582281|false|[0.60015536615859...| 2.762047335548208|\n",
      "|[0.385262401,3.66...|1.7316555|false|[-0.7470113808369...|1.9519164872808261|\n",
      "|[1.446918983,3.12...|1.7664417|false|[0.10737845154330...|2.1151631356846217|\n",
      "|[-0.400477567,3.8...|1.8164521|false|[-1.3793516789484...| 1.809175290412458|\n",
      "|[0.182321557,3.80...| 2.008214|false|[-0.9103321727276...| 1.976599848919253|\n",
      "|[0.009950331,3.26...|2.0215476|false|[-1.0490514397545...|1.3337244866291649|\n",
      "|[1.30833282,4.119...|2.0856721|false|[-0.0041515955390...|2.8276228974159805|\n",
      "|[1.442201993,3.68...|2.3075726|false|[0.10358235743550...| 2.148407475819118|\n",
      "|[1.771556762,3.89...|2.3749058|false|[0.36863733910332...|2.2512746976085585|\n",
      "|[1.16315081,4.035...|2.5687881|false|[-0.1209897852998...| 2.803305900954104|\n",
      "|[1.745715531,3.49...|2.5915164|false|[0.34784108031595...| 2.563896550699384|\n",
      "|[1.220829921,3.56...|2.5915164|false|[-0.0745713418835...|2.2565450520451744|\n",
      "|[0.512823626,3.63...|2.6844403|false|[-0.6443538779364...|2.5205033957609206|\n",
      "|[2.12704052,4.121...|2.6912431|false|[0.65472015869018...|3.1768739599801954|\n",
      "|[3.153590358,3.51...|2.7047113|false|[1.48085712132878...|3.1116854217513534|\n",
      "|[0.97455964,2.865...|2.7880929|false|[-0.2727623837621...| 1.577533998392557|\n",
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "rrPred = rrModel.transform(df_testing_scaled)\n",
    "rrPred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "MSE = 0.526516\n",
      "R squared = 0.498386\n",
      "Std Error = 0.185974\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(rrPred, {evaluator.metricName: \"mse\"})\n",
    "r2 = evaluator.evaluate(rrPred, {evaluator.metricName: \"r2\"})\n",
    "sde = (rrPred.rdd.map(lambda x: (x.prediction - x.label)**2).variance() / (rrPred.count() - 1))**0.5\n",
    "print('Test Summary:')\n",
    "print(\"MSE = %g\" % mse)\n",
    "print(\"R squared = %g\" % r2)\n",
    "print('Std Error = %g' % sde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main concepts in Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. This section covers the key concepts introduced by the Pipelines API, where the pipeline concept is mostly inspired by the scikit-learn project.\n",
    "\n",
    "[DataFrame](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#dataframe): This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n",
    "\n",
    "[Transformer](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#transformers): A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "[Estimator](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#estimators): An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "[Pipeline](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#pipeline): A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "[Parameter](https://spark.apache.org/docs/2.2.0/ml-pipeline.html#parameters): All Transformers and Estimators now share a common API for specifying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "<img src=\"https://spark.apache.org/docs/2.2.0/img/ml-Pipeline.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "\n",
    "### PipelineModel\n",
    "<img src=\"https://spark.apache.org/docs/2.2.0/img/ml-PipelineModel.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# extract principal components\n",
    "pca = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "# Ridge regression\n",
    "rr = LinearRegression(featuresCol='features', standardization=False)\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: scaler, pca, and rr.\n",
    "pipeline = Pipeline(stages=[scaler, pca, rr])\n",
    "\n",
    "# build the params grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(pca.k, [2, 3, 4]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName='mse'),\n",
    "                          numFolds=2)  # use more folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "cvPred = cvModel.transform(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "MSE = 0.521274\n",
      "R squared = 0.50338\n",
      "Std Error = 0.178724\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(cvPred, {evaluator.metricName: \"mse\"})\n",
    "r2 = evaluator.evaluate(cvPred, {evaluator.metricName: \"r2\"})\n",
    "sde = (cvPred.rdd.map(lambda x: (x.prediction - x.label)**2).variance() / (cvPred.count() - 1))**0.5\n",
    "print('Test Summary:')\n",
    "print(\"MSE = %g\" % mse)\n",
    "print(\"R squared = %g\" % r2)\n",
    "print('Std Error = %g' % sde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    " * [Data Types - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-data-types.html)\n",
    " * [Linear Methods - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#mjx-eqn-eqregPrimal)\n",
    " * [Basic Statistics](https://spark.apache.org/docs/2.2.0/ml-statistics.html)\n",
    " * [Extracting, transforming and selecting features](https://spark.apache.org/docs/2.2.0/ml-features.html)\n",
    " * [ML Pipelines](https://spark.apache.org/docs/2.2.0/ml-pipeline.html)\n",
    " * [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

